{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import Bunch"
      ],
      "metadata": {
        "id": "JCoYMVf_HPHT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ePL-bjmn0thW"
      },
      "outputs": [],
      "source": [
        "#data feature\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage.feature import graycomatrix, graycoprops\n",
        "from skimage.measure import regionprops\n",
        "from skimage.morphology import binary_erosion\n",
        "from scipy.special import comb\n",
        "import numpy.fft as fft\n",
        "\n",
        "\n",
        "# get horitonzal edges from sobelY\n",
        "def number_of_edgePixels(image):\n",
        "    sobelY = cv2.Sobel(image, cv2.CV_64F,0,1,ksize=5)\n",
        "    return np.count_nonzero(sobelY==255)\n",
        "\n",
        "# std of mean of rows, get high value for black/white horisontal stripes.\n",
        "def std_meanOfRows(image):\n",
        "    mean_row = np.mean(image, axis=1)\n",
        "    return np.std(mean_row, dtype=np.float32)\n",
        "\n",
        "# Computes the Laplacian of the image and returns the mean value of all the pixels. \n",
        "# A higher value indicates more edges and details in the image\n",
        "def mean_laplacian(image):\n",
        "    return cv2.mean(cv2.Laplacian(image, cv2.CV_64F))[0]\n",
        "\n",
        "# mean number of peak and valleys in each column, get high value for noisy pictures.\n",
        "def mean_PeaksValleys(image):\n",
        "    peaks_valleys = []\n",
        "    for j in range(image.shape[1]):\n",
        "        col = image[:, j]\n",
        "        num_peaks, num_valleys = count_peaks_valleys(col)\n",
        "        mean_peaks_valleys = (num_peaks + num_valleys) / 2\n",
        "        peaks_valleys.append(mean_peaks_valleys)\n",
        "    return np.mean(peaks_valleys, dtype=np.float32)\n",
        "    \n",
        "def count_peaks_valleys(arr_1d):\n",
        "    num_peaks = 0\n",
        "    num_valleys = 0\n",
        "    for i in range(1, len(arr_1d) - 1):\n",
        "        if arr_1d[i] > arr_1d[i-1] and arr_1d[i] > arr_1d[i+1]:\n",
        "            num_peaks += 1\n",
        "        elif arr_1d[i] < arr_1d[i-1] and arr_1d[i] < arr_1d[i+1]:\n",
        "            num_valleys += 1\n",
        "    return (num_peaks, num_valleys)\n",
        "\n",
        "# numbers of edges from canny edge detection, use dfs to explore number of group of edge pixels\n",
        "def num_edges_canny(image, L2Gradient, sobel_kernal_size):\n",
        "    T_lower = 100\n",
        "    T_upper = 200 \n",
        "    edge = cv2.Canny(image, T_lower, T_upper, apertureSize=sobel_kernal_size, L2Gradient = L2Gradient)\n",
        "    def dfs(r, c):\n",
        "        if r < 0 or r >= np.size(image, 0) or c < 0 or c >= np.size(image, 1) or image[r][c] == 0:\n",
        "            return 0\n",
        "            \n",
        "        image[r][c] = 0\n",
        "            \n",
        "        for i, j in zip((r - 1, r + 1, r, r), (c, c, c - 1, c + 1)):\n",
        "            dfs(i, j)\n",
        "            \n",
        "        return 1\n",
        "    \n",
        "    return sum(dfs(i, j) for i in range(np.size(image, 0)) for j in range(np.size(image, 1)))\n",
        "\n",
        "def lbp_histogram(image, radius, bins):\n",
        "    # compute the LBP histogram of the image\n",
        "    n_points = 8 * radius\n",
        "    lbp = local_binary_pattern(image, n_points, radius, method='uniform')\n",
        "    hist, _ = np.histogram(lbp, bins= bins, density=True)\n",
        "    return np.ravel(hist)\n",
        "\n",
        "# Haralick features \n",
        "# describe the texture of an image by computing statistics from the gray-level co-occurrence matrix (GLCM) of the image.\n",
        "def haralick_contrast(image, distance=1, angle=0):\n",
        "    glcm = graycomatrix(image, [distance], [angle], levels=256, symmetric=True, normed=True)\n",
        "    return graycoprops(glcm, 'contrast')[0, 0]\n",
        "\n",
        "#Hu moments are a set of shape features that are invariant to rotation, scale, and translation. \n",
        "# They can be computed from the moments of an image's contour\n",
        "def hu_moments(image):\n",
        "    moments = cv2.moments(image)\n",
        "    hu_moments = cv2.HuMoments(moments)\n",
        "    return np.ravel(hu_moments)\n",
        "\n",
        "#set of shape features that are invariant to rotation, scale, and translation. \n",
        "# They can be computed from the radial and angular moments of an image's contour\n",
        "def zernike_moments(image, radius=1, degree=8):\n",
        "    eroded_image = binary_erosion(image, np.ones((2*radius+1, 2*radius+1)))\n",
        "    props = regionprops(eroded_image.astype(np.uint8))\n",
        "    moments = np.zeros(degree+1)\n",
        "    for prop in props:\n",
        "        r = np.sqrt(prop.area/np.pi)\n",
        "        if r < radius:\n",
        "            z = np.complex(prop.centroid[1], prop.centroid[0])\n",
        "            for n in range(degree+1):\n",
        "                for m in range(-n, n+1, 2):\n",
        "                    if m < 0:\n",
        "                        cmn = comb(n, int((n-m)/2))\n",
        "                        fm = np.exp(np.complex(0, m*prop.orientation))\n",
        "                        moments[n] += (r**n) * cmn\n",
        "\n",
        "#GLCM-based texture features\n",
        "def glcm_features(image):\n",
        "    # Compute the gray-level co-occurrence matrix\n",
        "    glcm = graycomatrix(image, [5], [0, np.pi/4, np.pi/2, 3*np.pi/4])\n",
        "    \n",
        "    # Compute some commonly used GLCM features\n",
        "    contrast = graycoprops(glcm, 'contrast')\n",
        "    dissimilarity = graycoprops(glcm, 'dissimilarity')\n",
        "    homogeneity = graycoprops(glcm, 'homogeneity')\n",
        "    energy = graycoprops(glcm, 'energy')\n",
        "    correlation = graycoprops(glcm, 'correlation')\n",
        "    \n",
        "    # Concatenate the features into a single vector\n",
        "    features = np.concatenate([contrast, dissimilarity, homogeneity, energy, correlation])\n",
        "    \n",
        "    return np.ravel(features)\n",
        "\n",
        "\n",
        "# Fourier descriptors represent the shape of an object in terms of its frequency components\n",
        "def fourier_shape_features(image):\n",
        "    # Compute the Fourier Transform of the image\n",
        "    f = fft.fft2(image)\n",
        "    \n",
        "    # Shift the zero frequency component to the center\n",
        "    fshift = fft.fftshift(f)\n",
        "    \n",
        "    # Take the magnitude of the Fourier Transform\n",
        "    magnitude_spectrum = np.log(np.abs(fshift))\n",
        "    \n",
        "    # Extract the Fourier coefficients for the first 5 frequencies in each dimension\n",
        "    n = image.shape[0]\n",
        "    m = image.shape[1]\n",
        "    p = 5\n",
        "    descriptors = np.zeros((p, p))\n",
        "    for i in range(p):\n",
        "        for j in range(p):\n",
        "            descriptors[i, j] = np.abs(fshift[n//2 + i, m//2 + j])\n",
        "    \n",
        "    # Flatten the descriptors into a single vector\n",
        "    features = descriptors.flatten()\n",
        "    return np.ravel(features)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.utils import Bunch\n"
      ],
      "metadata": {
        "id": "_ZqayDRD3Rht"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #example of using features to train model\n",
        "# number_image = 2910\n",
        "# feature = 9\n",
        "# img=np.zeros(shape=(number_image,feature))\n",
        "# for i in range(0,number_image): \n",
        "#     img[i][0] = number_of_edgePixels(npdata['data'][i])\n",
        "#     img[i][1] = std_meanOfRows(npdata['data'][i])\n",
        "#     img[i][2] = mean_PeaksValleys(npdata['data'][i])\n",
        "#     img[i][3] = np.std(npdata['data'][i])\n",
        "#     img[i][4] = mean_laplacian(npdata['data'][i])\n",
        "#     img[i][5] = hu_moments(npdata['data'][i])[0]\n",
        "#     img[i][6] = zernike_moments(npdata['data'][i])\n",
        "#     img[i][7] = glcm_features(npdata['data'][i])[0][0]\n",
        "#     img[i][8] = fourier_shape_features(npdata['data'][i])[0]\n",
        "#     #print(fourier_shape_features(npdata['data'][i]).shape)\n",
        "    \n",
        "# print(img.shape)\n",
        "# dataset = Bunch(data = img, target=npdata['label'])"
      ],
      "metadata": {
        "id": "aqezqG1X02hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_tree_model(dataSet, testData_percentage, max_depth):\n",
        "    img_train, img_test, label_train, label_test = train_test_split(dataSet.data, dataSet.target, test_size=testData_percentage, random_state=0)\n",
        "    random_state = 0\n",
        "    classifier = DecisionTreeClassifier(max_depth = 4, \n",
        "                             random_state = 0)\n",
        "    \n",
        "    #fitting classifier model\n",
        "    classifier.fit(img_train, label_train)\n",
        "    \n",
        "    # classification_report outputs classification metrics\n",
        "    # such as precision, recall and F1 score\n",
        "    pred_result = classifier.predict(img_train)\n",
        "    print('Classification Training Report: \\n', classification_report(label_train, pred_result))\n",
        "    \n",
        "    # confusion_matrix outputs how many samples are correctly or incorrectly classified\n",
        "    print('Train Confusion Matrix: \\n', confusion_matrix(label_train, pred_result), \"\\n\")\n",
        "\n",
        "    # accuracy computes classification accuracy\n",
        "    print('Train Accuracy: ', accuracy_score(label_train, pred_result), '\\n')\n",
        "    \n",
        "    # testing with validate data\n",
        "    validate_result = classifier.predict(img_test)\n",
        "    print('Classification Testing Report: \\n', classification_report(label_test, validate_result, zero_division=0))\n",
        "    # `confusion_matrix` outputs how many samples are correctly or incorrectly classified\n",
        "    print('Test Confusion Matrix: \\n', confusion_matrix(label_test, validate_result), \"\\n\")\n",
        "    # `accuracy` computes classification accuracy\n",
        "    print('Test Accuracy: ', accuracy_score(label_test, validate_result))"
      ],
      "metadata": {
        "id": "8NLkVzPt7_TZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from .npz file\n",
        "npdata = np.load(f'/content/1776_datasetA_200_40.npz')\n",
        "#npdata = np.load(f'/content/2910_datasetC_200x200.npz')\n",
        "print(npdata['data'].shape)\n",
        "\n",
        "number_image = 1776\n",
        "radius = 8\n",
        "bins = 8\n",
        "total_feature_size = bins + 57\n",
        "\n",
        "img_features=np.zeros(shape=(number_image, total_feature_size)) \n",
        "#print(fourier_shape_features(npdata['data'][0]).shape)\n",
        "#img_features[i][20:] =  zernike_moments(npdata['data'][i])\n",
        "\n",
        "for i in range(0,number_image):\n",
        "     img_features[i][:bins] = lbp_histogram(npdata['data'][i], radius, bins)\n",
        "     img_features[i][8] = number_of_edgePixels(npdata['data'][i])\n",
        "     img_features[i][9] = std_meanOfRows(npdata['data'][i])\n",
        "     img_features[i][10] =  mean_PeaksValleys(npdata['data'][i])\n",
        "     img_features[i][11] = np.std(npdata['data'][i])\n",
        "     img_features[i][12] = mean_laplacian(npdata['data'][i])\n",
        "     img_features[i][13:20] = hu_moments(npdata['data'][i])[0:7]\n",
        "     img_features[i][20:40] = glcm_features(npdata['data'][i])[0:20]\n",
        "     img_features[i][40:65] = fourier_shape_features(npdata['data'][i])[0:25]\n",
        "\n",
        "print(img_features.shape)\n",
        "\n",
        "#add depth to image features\n",
        "img_features = np.insert(img_features, bins, npdata['depth'], axis=1)\n",
        "print(img_features.shape)\n",
        "\n",
        "# make dataset in Bunch format \n",
        "dataset = Bunch(data = img_features, target=npdata['label'])\n",
        "\n",
        "\n",
        "decision_tree_model(dataset, 0.2, 4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAe3ISv58Cn0",
        "outputId": "28d4cc51-606c-4fe2-e081-b714d22da83c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1776, 200, 200)\n",
            "(1776, 65)\n",
            "(1776, 66)\n",
            "Classification Training Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.83      0.89       837\n",
            "           1       0.79      0.94      0.86       583\n",
            "\n",
            "    accuracy                           0.87      1420\n",
            "   macro avg       0.87      0.88      0.87      1420\n",
            "weighted avg       0.89      0.87      0.88      1420\n",
            "\n",
            "Train Confusion Matrix: \n",
            " [[695 142]\n",
            " [ 36 547]] \n",
            "\n",
            "Train Accuracy:  0.8746478873239436 \n",
            "\n",
            "Classification Testing Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.83      0.87       206\n",
            "           1       0.79      0.91      0.84       150\n",
            "\n",
            "    accuracy                           0.86       356\n",
            "   macro avg       0.86      0.87      0.86       356\n",
            "weighted avg       0.87      0.86      0.86       356\n",
            "\n",
            "Test Confusion Matrix: \n",
            " [[170  36]\n",
            " [ 14 136]] \n",
            "\n",
            "Test Accuracy:  0.8595505617977528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def decision_tree_model(dataSet, testData_percentage):\n",
        "    img_train, img_test, label_train, label_test = train_test_split(dataSet.data, dataSet.target, test_size=testData_percentage, random_state=0)\n",
        "    random_state = 0\n",
        "    \n",
        "    # Define the hyperparameters to tune and their possible values\n",
        "    parameters = {'max_depth': [2, 4, 6, 8, 10, 12],\n",
        "                  'criterion': ['gini', 'entropy']}\n",
        "    \n",
        "    # Create a Decision Tree Classifier object\n",
        "    dt_classifier = DecisionTreeClassifier(random_state=random_state)\n",
        "    \n",
        "    # Create a Grid Search object\n",
        "    grid_search = GridSearchCV(estimator=dt_classifier,\n",
        "                               param_grid=parameters,\n",
        "                               scoring='accuracy',\n",
        "                               cv=5)\n",
        "    \n",
        "    # Fit the Grid Search object to the data\n",
        "    grid_search.fit(img_train, label_train)\n",
        "    \n",
        "    # Get the best hyperparameters and their corresponding accuracy\n",
        "    best_params = grid_search.best_params_\n",
        "    best_accuracy = grid_search.best_score_\n",
        "    print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
        "    print(\"Best Accuracy: \", grid_search.best_score_)\n",
        "    \n",
        "    # Train the Decision Tree Classifier with the best hyperparameters\n",
        "    classifier = DecisionTreeClassifier(max_depth=best_params['max_depth'], \n",
        "                                         criterion=best_params['criterion'],\n",
        "                                         random_state=random_state)\n",
        "    \n",
        "    # Fit the classifier model\n",
        "    classifier.fit(img_train, label_train)\n",
        "    \n",
        "    # Classification report outputs classification metrics such as precision, recall and F1 score\n",
        "    pred_result = classifier.predict(img_train)\n",
        "    print('Classification Training Report: \\n', classification_report(label_train, pred_result))\n",
        "    \n",
        "    # Confusion_matrix outputs how many samples are correctly or incorrectly classified\n",
        "    print('Train Confusion Matrix: \\n', confusion_matrix(label_train, pred_result), \"\\n\")\n",
        "\n",
        "    # Accuracy computes classification accuracy\n",
        "    print('Train Accuracy: ', accuracy_score(label_train, pred_result), '\\n')\n",
        "    \n",
        "    # Testing with validate data\n",
        "    validate_result = classifier.predict(img_test)\n",
        "    print('Classification Testing Report: \\n', classification_report(label_test, validate_result, zero_division=0))\n",
        "    \n",
        "    # Confusion_matrix outputs how many samples are correctly or incorrectly classified\n",
        "    print('Test Confusion Matrix: \\n', confusion_matrix(label_test, validate_result), \"\\n\")\n",
        "    \n",
        "    # Accuracy computes classification accuracy\n",
        "    print('Test Accuracy: ', accuracy_score(label_test, validate_result))\n",
        "\n",
        "    return classifier\n"
      ],
      "metadata": {
        "id": "MqvvsUTJuwLu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_tree_model(dataset, 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "8nh4xbr_6kZ8",
        "outputId": "bd2144a3-01cd-4699-e465-17c6218869f0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:  {'criterion': 'entropy', 'max_depth': 10}\n",
            "Best Accuracy:  0.8338028169014085\n",
            "Classification Training Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97       837\n",
            "           1       0.96      0.94      0.95       583\n",
            "\n",
            "    accuracy                           0.96      1420\n",
            "   macro avg       0.96      0.96      0.96      1420\n",
            "weighted avg       0.96      0.96      0.96      1420\n",
            "\n",
            "Train Confusion Matrix: \n",
            " [[817  20]\n",
            " [ 35 548]] \n",
            "\n",
            "Train Accuracy:  0.9612676056338029 \n",
            "\n",
            "Classification Testing Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87       206\n",
            "           1       0.83      0.82      0.82       150\n",
            "\n",
            "    accuracy                           0.85       356\n",
            "   macro avg       0.85      0.85      0.85       356\n",
            "weighted avg       0.85      0.85      0.85       356\n",
            "\n",
            "Test Confusion Matrix: \n",
            " [[180  26]\n",
            " [ 27 123]] \n",
            "\n",
            "Test Accuracy:  0.851123595505618\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=0)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=10, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=10, random_state=0)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def select_features(dataset, k):\n",
        "    # Split the dataset into training and testing sets\n",
        "    img_train, img_test, label_train, label_test = train_test_split(dataset.data, dataset.target, test_size=0.2, random_state=0)\n",
        "    \n",
        "    # Train a decision tree model using the full set of features\n",
        "    classifier = DecisionTreeClassifier(max_depth = 10, random_state = 0)\n",
        "    classifier.fit(img_train, label_train)\n",
        "    \n",
        "    # Get the feature importances\n",
        "    feature_importances = classifier.feature_importances_\n",
        "    \n",
        "    # Sort the features by their importance\n",
        "    sorted_idx = np.argsort(feature_importances)[::-1]\n",
        "    \n",
        "    # Select the top k features\n",
        "    selected_features = sorted_idx[:k]\n",
        "    \n",
        "    # Train a new decision tree model using only the selected features\n",
        "    classifier_selected = DecisionTreeClassifier(max_depth = 4, random_state = 0)\n",
        "    classifier_selected.fit(img_train[:, selected_features], label_train)\n",
        "    \n",
        "    # Evaluate the model on the testing set\n",
        "    pred_result = classifier_selected.predict(img_test[:, selected_features])\n",
        "    print('Classification Testing Report: \\n', classification_report(label_test, pred_result, zero_division=0))\n",
        "    print('Test Confusion Matrix: \\n', confusion_matrix(label_test, pred_result), \"\\n\")\n",
        "    print('Test Accuracy: ', accuracy_score(label_test, pred_result))\n",
        "    \n",
        "    # Return the selected features\n",
        "    return selected_features\n"
      ],
      "metadata": {
        "id": "UA6YZgNO7IJd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = select_features(dataset, 15)\n",
        "print(selected_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfVTTe8q7jPk",
        "outputId": "701de03e-f366-4230-facc-8696ac0cd990"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Testing Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.82      0.87       206\n",
            "           1       0.78      0.92      0.85       150\n",
            "\n",
            "    accuracy                           0.86       356\n",
            "   macro avg       0.86      0.87      0.86       356\n",
            "weighted avg       0.87      0.86      0.86       356\n",
            "\n",
            "Test Confusion Matrix: \n",
            " [[168  38]\n",
            " [ 12 138]] \n",
            "\n",
            "Test Accuracy:  0.8595505617977528\n",
            "[ 8 21 65 63 54  1 10 64 60 28 61 53 55 42 24]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def select_features(dataset, k):\n",
        "    # Split the dataset into training and testing sets\n",
        "    img_train, img_test, label_train, label_test = train_test_split(dataset.data, dataset.target, test_size=0.2, random_state=0)\n",
        "    \n",
        "    # Train a decision tree model using the full set of features\n",
        "    classifier = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
        "    classifier.fit(img_train, label_train)\n",
        "    \n",
        "    # Get the feature importances\n",
        "    feature_importances = classifier.feature_importances_\n",
        "    \n",
        "    # Sort the features by their importance\n",
        "    sorted_idx = np.argsort(feature_importances)[::-1]\n",
        "    \n",
        "    # Select the top k features\n",
        "    selected_features = sorted_idx[:k]\n",
        "    \n",
        "    # Train a new decision tree model using only the selected features\n",
        "    classifier_selected = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
        "    classifier_selected.fit(img_train[:, selected_features], label_train)\n",
        "    \n",
        "    # Evaluate the model on the testing set\n",
        "    pred_result = classifier_selected.predict(img_test[:, selected_features])\n",
        "    print('Classification Testing Report: \\n', classification_report(label_test, pred_result, zero_division=0))\n",
        "    print('Test Confusion Matrix: \\n', confusion_matrix(label_test, pred_result), \"\\n\")\n",
        "    print('Test Accuracy: ', accuracy_score(label_test, pred_result))\n",
        "    \n",
        "    # Return the selected features and the trained model\n",
        "    return selected_features, classifier_selected, img_test\n"
      ],
      "metadata": {
        "id": "r-k0DJQF9LLi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features, classifier_selected, img_test = select_features(dataset, 15)\n",
        "test_predictions = classifier_selected.predict(img_test[:, selected_features])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDg7_h_e9QkA",
        "outputId": "bfe2fced-99d5-431e-b14c-07f9b6e69a02"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Testing Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.83      0.87       206\n",
            "           1       0.79      0.91      0.84       150\n",
            "\n",
            "    accuracy                           0.86       356\n",
            "   macro avg       0.86      0.87      0.86       356\n",
            "weighted avg       0.87      0.86      0.86       356\n",
            "\n",
            "Test Confusion Matrix: \n",
            " [[170  36]\n",
            " [ 14 136]] \n",
            "\n",
            "Test Accuracy:  0.8595505617977528\n"
          ]
        }
      ]
    }
  ]
}